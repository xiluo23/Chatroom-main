# C++ 高性能集群聊天服务器 - 面试突击宝典 (结合简历与项目)

这份文档专门针对你在简历中提到的“高性能”、“集群”、“Nginx负载均衡”以及“解决高并发瓶颈”的经历，整理了面试官最可能追问的深层问题。

---

## 第一部分：项目介绍 (自我介绍模板)

**面试官**: "请介绍一下你的这个聊天服务器项目。"

**参考话术**:
> "这是一个基于 **C++ Linux** 环境开发的**高性能分布式集群聊天服务器**。
> 它的核心架构采用了 **Reactor 模型**（Epoll + 非阻塞 I/O），业务处理使用了**自定义线程池**，数据存储层实现了 **MySQL 数据库连接池**。
> 为了支持水平扩展（Clustering），我引入了 **Nginx** 进行 TCP 负载均衡，并使用 **Redis 发布/订阅模式** 来处理跨服务器的消息路由。
> 在压测阶段，我成功解决了**线程安全（strtok_r）**、**文件描述符限制**以及**数据库连接瓶颈**等问题，最终实现了高并发下的稳定运行。"

---

## 第二部分：简历核心点深度拷问

### 1. 关于 "集群" 与 "Nginx 负载均衡"
**简历写了**: "使用 Nginx 进行 TCP 负载均衡..."

*   **Q1: Nginx 在这里是工作在第几层？它是如何配置的？**
    *   **A**: Nginx 工作在**传输层 (Layer 4)**，使用了 `stream` 模块（而不是 HTTP 的 `http` 模块）。配置了 `upstream` 指向多个后端服务器（如 `127.0.0.1:8000`, `127.0.0.1:8001`），默认使用**轮询 (Round Robin)** 算法将客户端的 TCP 连接分发到不同的服务器实例。

*   **Q2: 如果用户 A 连接在服务器 1，用户 B 连接在服务器 2，他们如何通信？**
    *   **A**: 这就是引入 **Redis** 的原因。服务器之间不直接互连，而是通过 Redis 解耦。
        *   用户 B 上线时，会订阅 `user_id_B` 的频道。
        *   用户 A 发消息给 B 时，服务器 1 发现 B 不在本地，就向 Redis 的 `user_id_B` 频道 **Publish** 消息。
        *   服务器 2 收到订阅通知，解析消息并通过 socket 发送给用户 B。

### 2. 关于 "高并发" 与 "线程池"
**简历写了**: "实现线程池处理业务逻辑..."

*   **Q3: 既然用了 Epoll (IO 多路复用)，为什么还需要线程池？**
    *   **A**: Epoll (Reactor) 负责高效地**接入连接**和**读取数据**（IO 密集型）。但具体的业务逻辑（如查数据库、密码加密、JSON解析）是 CPU 或 阻塞式 IO 密集型的。如果不把这些任务扔给线程池，主线程就会被阻塞，导致无法响应其他客户端的新连接或消息，从而降低整体并发度。

*   **Q4: 你的线程池线程数设置了多少？依据是什么？**
    *   **A**: 我目前设置了 **16** 个线程（之前是 8，扩容以解决瓶颈）。
    *   **依据**: 主要是考虑到业务中有大量的**数据库操作**（IO 密集型）。通常设置为 `CPU核心数 * 2` 到 `CPU核心数 * 4` 甚至更多，取决于 IO 等待时间的比例。经过压测观察，增加线程数有效缓解了任务队列积压。

### 3. 关于 "数据库连接池"
**简历写了**: "封装 MySQL 连接池..."

*   **Q5: 为什么不使用开源的连接池（如 MyBatis, HikariCP 思想的 C++ 实现）而要自己写？**
    *   **A**: 为了更深入理解**资源复用**和**RAII (资源获取即初始化)** 技术。我自己实现了一个基于 `std::queue` 的连接池，并利用 C++ 的构造/析构函数机制（RAII）封装了 `DbConnectionGuard`，确保连接在使用完后能自动归还，即使发生异常也不会泄露。

---

## 第三部分：项目难点与排查经历 (Star 亮点)

**面试官**: "在这个项目中你遇到了什么困难？是如何解决的？"
*(这是你简历中最亮眼的部分，必须按这个逻辑讲)*

### 案例 1：突破 1024 连接数限制 (系统层面)
*   **现象**: 压测初期，并发量达到 **1000** 左右时，新客户端无法连接，报错 "Too many open files"。
*   **分析**: Linux 默认的进程最大文件描述符限制 (ulimit) 通常是 **1024**。这成为了系统的第一道瓶颈。
*   **解决**:
    1.  在代码中引入 `<sys/resource.h>`。
    2.  使用 `setrlimit` 系统调用，在程序启动时自动将 `RLIMIT_NOFILE` (软/硬限制) 提升到 **65535**。
*   **结果**: 解除限制后，并发连接数成功突破 1000，达到了 **2000+** (随后遇到了数据库瓶颈，见案例 2)。

### 案例 2：数据库连接耗尽 (架构层面)
*   **现象**: 即使提升了 ulimit，在大量用户同时**注册/登录**时，服务器响应极慢，日志显示大量数据库操作排队。
*   **分析**: 初始设计的线程池只有 32 个线程，对应 32 个数据库连接。当 2000 个用户并发请求时，数据库连接成为争抢最激烈的资源。
*   **解决**: 将线程池和数据库连接池扩容至 **64**（未来可配置化），大幅减少了任务在队列中的等待时间。

### 进一步优化方案 (Bonus)
如果面试官追问“还能怎么优化”，可以提出以下几点：
1.  **读写分离**: 数据库层面实现主从复制，写操作走主库，大量的读操作（如获取历史消息）走从库。
2.  **缓存层 (Redis)**: 目前 Redis 仅用于消息路由。可以将用户信息、在线状态、热门聊天记录等热点数据缓存到 Redis 中，减少对 MySQL 的直接查询。
3.  **连接池动态伸缩**: 实现一个支持动态扩缩容的连接池，而不是固定 64 个。
4.  **异步数据库驱动**: 考虑使用异步 MySQL 驱动，避免工作线程被数据库 IO 阻塞。

### 3. 广播消息乱码/崩溃 (线程安全代码层面)
*   **现象**: 在多线程进行广播消息（Broadcast）时，发现部分用户的 ID 解析错误，甚至导致逻辑错乱。
*   **分析**: 我使用了 `strtok` 函数分割字符串。后来查阅文档发现 `strtok` 内部使用静态变量保存状态，是**线程不安全**的。在多线程并发处理广播任务时，上下文被覆盖了。
*   **解决**: 将所有 `strtok` 替换为 **`strtok_r`** (Reentrant 版本)，自己维护 `saveptr` 指针，确保了线程安全。

---

## 第四部分：手撕代码/底层原理追问

*   **Q: 你的 Epoll 是 ET 还是 LT？**
    *   **A**: 注册事件使用默认的 **LT (水平触发)**。但为了高效，我在读取数据时采用了类似 ET 的 `while(1)` 循环读取直到 `EAGAIN`。这样既保证了数据的完整读取，又避免了 ET 模式下容易丢失事件的编程复杂度。

*   **Q: 只有 `send` 和 `recv` 吗？怎么处理粘包？**
    *   **A**: 我设计了应用层协议（Header + Body），并在服务端为每个 socket 维护了一个 `ClientBuffer`。Reactor 收到数据先追加到 Buffer，然后由专门的逻辑（Parser）去检查 Buffer 中是否包含完整的数据包（根据 Header 中的长度字段）。

*   **Q: 为什么数据库查询要加锁？**
    *   **A**: 在我的连接池实现中，`db_pool` 是一个共享队列，多线程取连接时必须加锁 (`pthread_mutex`)。但拿到连接后的 `SQL` 执行过程是不需要应用层加锁的（MySQL 客户端库通常是线程安全的，或者是每个线程独占一个连接对象，互不干扰）。
